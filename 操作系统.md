## CPU的用户态和内核态和内存的用户空间内核空间?
谈到CPU的这两个工作状态，也就是处理器的这两个工作状态，那我们有必要说一下为什么搞出这两个鬼玩意出来。  
<p style="text-indent:2em">用过电脑的娃娃们肯定知道在一个系统中既有操作系统的程序，也由普通用户的程序。但那么多指令，可不是随便乱用的，有些指令只能由系统来使用，禁止用户程序去直接访问。为了保证操作系统和各个应用程序能够顺利运行，就必须对他们进行限制，否则的话就根本没有办法保证系统的安全性和稳定。  
<p style="text-indent:2em">所以呢，根据运行程序对资源和机器指令的使用权限，把处理器设置为不同的状态。多数系统把处理器的工作状态分为管态和目态两种。也就是我们今天要说的这两个东西。  
<p style="text-indent:2em">所谓管态，即操作系统的管理程序运行时的状态，它具有较高的特权级别，也称为特权态、系统态、内核态或者核心态。当处理器处于管态时，他可以执行所有的指令，包括各种特权指令，也可以使用所有的资源，并且具有改变处理器状态的能力，是感觉很牛逼。需要指出的是，管态和超级用户不同，前者是指CPU的状态，后者是指一种特殊的计算机用户；前者主要是从硬件的角度去执行任何指令，而后者是从软件的角度来管理系统的软硬件资源，如用户账户、权限管理、文件访问等。超级用户执行的程序不一定运行在管态，而管态程序也不一定由系统管理员启动，普通用户也可以启动。
<p style="text-indent:2em">所谓目态，即用户程序运行时的状态，它具有较低的特权级别，又称为普通态或用户态。在这种状态下不能使用特权指令，不能直接使用系统资源，也不能改变CPU的工作状态，并且只能访问这个用户程序自己的存储空间。用户态不允许程序进行处理器中要求特权态的操作，以避免操作系统崩溃。每个进程都在各自的用户空间中运行，而不允许存取其他程序的用户空间。
   
![](media/32bj234j234xcv.jpg)   
<p style="text-indent:2em">当一个任务(进程)执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态(或简称为内核态)。此时处理器处于特权级最高的(0级)内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态(用户态)。即此时处理器在特权级最低的(3级)用户代码中运行。
<p style="text-indent:2em">在内核态下CPU可执行任何指令，在用户态下CPU只能执行非特权指令。当CPU处于内核态，可以随意进入用户态；而当CPU处于用户态时，用户从用户态切换到内核态只有在系统调用和中断两种情况下发生，一般程序一开始都是运行于用户态，当程序需要使用系统资源时，就必须通过调用软中断进入内核 态。 　　
<p style="text-indent:2em">Linux使用了Ring3级别运行用户态，Ring0作为内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。Linux进程的4GB地址空间，3G-4G部分大家是共享的，是内核态的地址空间，这里存放在整个内核的代码和所有的内核模块， 以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执 行这些代码完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用。

## 用户进程缓冲区和内核缓冲区?
用户进程缓冲区  
用户进程通过系统调用访问系统资源的时候，需要切换到内核态，而这对应一些特殊的堆栈和内存环境，必须在系统调用前建立好。而在系统调用结束后，cpu会从核心模式切回到用户模式，而堆栈又必须恢复成用户进程的上下文。而这种切换就会有大量的耗时。为了优化这一情况，需要在应用程序中加入缓冲区，通过一次操作更多的数据来减少上下文切换。  
你看一些程序在读取文件时，会先申请一块内存数组，称为buffer，然后每次调用read，读取设定字节长度的数据，写入buffer。（用较小的次数填满buffer）。之后的程序都是从buffer中获取数据，当buffer使用完后，在进行下一次调用，填充buffer。  
所以说：用户缓冲区的目的是为了减少系统调用次数，从而降低操作系统在用户态与核心态切换所耗费的时间。

内核缓冲区  
除了在进程中设计缓冲区，内核也有自己的缓冲区。  
当一个用户进程要从磁盘读取数据时，内核一般不直接读磁盘，而是将内核缓冲区中的数据复制到进程缓冲区中。  
但若是内核缓冲区中没有数据，内核会把对数据块的请求，加入到请求队列，然后把进程挂起，为其它进程提供服务。  
等到数据已经读取到内核缓冲区时，把内核缓冲区中的数据读取到用户进程中，才会通知进程，当然不同的io模型，在调度和使用内核缓冲区的方式上有所不同。  
你可以认为，read是把数据从内核缓冲区复制到进程缓冲区。write是把进程缓冲区复制到内核缓冲区。  
当然，write并不一定导致内核的写动作，比如os可能会把内核缓冲区的数据积累到一定量后，再一次写入。这也就是为什么断电有时会导致数据丢失。  
所以说内核缓冲区，是为了在OS级别，提高磁盘IO效率，优化磁盘写操作。

## 关于epoll和select，POLL的区别？
都是IO复用，I/O多路复用就通过一种机制，可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。   
epoll的效率更高，优化了select的轮询操作，通过callback事件响应方式。   
epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。  
LT（level triggered）是缺省的工作方式，并且同时支持block和no-block socket.在这种做法中，内核告诉你一个文件描述符是否就绪了，然后你可以对这个就绪的fd进行IO操作。如果你不作任何操作，内核还是会继续通知你的，所以，这种模式编程出错误可能性要小一点。传统的select/poll都是这种模型的代表。   
ET （edge-triggered）是高速工作方式，只支持no-block socket。在这种模式下，当描述符从未就绪变为就绪时，内核通过epoll告诉你。然后它会假设你知道文件描述符已经就绪，并且不会再为那个文件描述符发送更多的就绪通知，直到你做了某些操作导致那个文件描述符不再为就绪状态了。  
ET和LT的区别就在这里体现，LT事件不会丢弃，而是只要读buffer里面有数据可以让用户读，则不断的通知你。而ET则只在事件发生之时通知。可以简单理解为LT是水平触发，而ET则为边缘触发。LT模式只要有事件未处理就会触发，而ET则只在高低电平变换时（即状态从1到0或者0到1）触发。  

select的几大缺点：  
（1）每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大。  
（2）同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大。  
（3）select支持的文件描述符数量太小了，默认是1024。  

poll实现  
poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用链表结构而不是select的数组结构。主要是增大了fd集合，但select的（1），（2）两个缺点还是无法解决。

epoll  
epoll和select和poll的调用接口上的不同，select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create,epoll_ctl和epoll_wait，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。  
　　对于第一个缺点，epoll的解决方案在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。  
　　对于第二个缺点，epoll的解决方案不像select或poll一样每次都把current轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把current挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表）。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd（利用schedule_timeout()实现睡一会，判断一会的效果，和select实现中的第7步是类似的）。  
　　对于第三个缺点，epoll没有这个限制，它所支持的FD上限是最大可以打开文件的数目，这个数字一般远大于2048,举个例子,在1GB内存的机器上大约是10万左右，具体数目可以cat /proc/sys/fs/file-max察看,一般来说这个数目和系统内存关系很大。  
使用mmap加速内核与用户空间的消息传递。无论是select,poll还是epoll都需要内核把FD消息通知给用户空间，如何避免不必要的内存拷贝就很重要，在这点上，epoll是通过内核于用户空间mmap同一块内存实现的。 
 
总结
select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。  
select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。

>[关于同步，异步，阻塞，非阻塞，IOCP/epoll,select/poll,AIO ,NIO ,BIO的总结](https://blog.csdn.net/chen8238065/article/details/48315085)

## 操作系统的页式存储？
内存分区存储管理的一个特点是连续性，每个程序都分有一片连续的内存区域。这种连续性导致碎片问题，包括固定分区中的内碎片和可变分区中的外碎片。为了解决这些问题，人们又提出了“页式存储管理方案”。  
它的基本出发点是打破存储分配的连续性，使一个程序的逻辑地址空间可以分布在若干个离散的内存块上，从而达到充分利用内存，提高内存利用率的作用。  
页式存储管理的基本思路是：一方面，把物理内存划分为许多个固定大小的内存块，称为物理页面，或页框；另一方面，把逻辑地址空间也划成大小相同的块，称为逻辑页面，页面的大小要求是2^n.一般在512个字节到8KB之间。  
当一个用户程序被装入内存时，不是以整个程序为单位，把它放在一整块连续的区域，而是以页面为单位来进行分配的。对于一个大小为N个页面的程序，需要有N个空闲的物理页面把它装进去，当然，这些物理页面不一定是连续的。  

**页**  
相对物理块来说，页是逻辑地址空间（虚拟内存空间）的划分，是逻辑地址空间顺序等分而成的一段逻辑空间，并依次连续编号。页的大小一般为 512B~8KB。  
例如：一个 32 位的操作系统，页的大小设为 2^12=4Kb，那么就有页号从 0 编到 2^20 的那么多页逻辑空间。  
**物理块**  
物理块则是相对于虚拟内存对物理内存按顺序等大小的划分。物理块的大小需要与页的大小一致。  
例如：2^31=2Gb 的物理内存，按照 4Kb/页的大小划分，可以划分成物理块号从 0 到 2^19 的那么多块的物理内存空间。  
**逻辑地址结构**  
页式存储管理中，逻辑地址可以解读成页号+地址偏移量（页内地址）。  
一个 32 位的逻辑地址，页大小设为 2^12=4Kb。高 20 位则是页号，低 12 位则是页内地址。  
**页表**  
页表是记录逻辑空间（虚拟内存）中每一页在内存中对应的物理块号。但并非每一页逻辑空间都会实际对应着一个物理块，只有实际驻留在物理内存空间中的页才会对应着物理块。  
页表是需要一直驻留在物理内存中的（多级页表除外），另外页表的起址和长度存放在 PCB（Process Control Block）进程控制结构体中。  
**快表(TLB，Translation Look aside Buffer)**  
快表是为了加快虚拟地址到物理地址这个转换过程而存在的。页式存储管理的快表一般存放在 CPU 内部的高速缓冲存储器 Cache。快表与页表的功能类似，其实就是将一部分页表存到 CPU 内部的高速缓冲存储器 Cache。CPU 寻址时先到快表查询相应的页表项形成物理地址，如果查询不到，则到内存中查询，并将对应页表项调入到快表中。但，如果快表的存储空间已满，则需要通过算法找到一个暂时不再需要的页表项，将它换出内存。因为高速缓冲存储器的访问速度要比内存的访问速度快很多，因此使用可以大大加快虚拟地址转换成物理地址。根据统计，快表的命中率可以达到 90%以上。
**一级页表的缺陷**  
由于页表必须连续存放，并且需要常驻物理内存，当逻辑地址空间很大时，导致页表占用内存空间很大。  
例如，地址长度 32 位，可以得到最大空间为 4GB，页大小 4KB，最多包含 4G/4K=1M 个页。若每个页表项占用 4 个字节，则页表最大长度为 4MB，即要求内存划分出连续 4MB 的空间来装载页表；若地址程度为 64 位时，就需要恐怖的 4*2^52Byte 空间来存储页表了。而且页表采用的是连续分配，不是分页分配。  
采用离散分配方式的管理页表，将当前需要的部分页表项调入内存，其余的页表项仍驻留在磁盘上，需要时再调入。  
**二级页表**  
二级页表即是对页表本身采用分页式管理，对页表本身增加了一层页表管理。页的大小就是一个页表的大小，一个页表只能装在一个页中。  
**多级页表**  
多级页表和二级页表类似。多级页表和二级页表是为了节省物理内存空间。使得页表可以在内存中离散存储。  
>[深入理解操作系统之——分页式存储管理](https://www.tomorrow.wiki/archives/334)

## 大叶内存？
系统进程是通过虚拟地址访问内存，但是CPU必须把它转换程物理内存地址才能真正访问内存。为了提高这个转换效率，CPU会缓存最近的虚拟内存地址和物理内存地址的映射关系，并保存在一个由CPU维护的映射表中。为了尽量提高内存的访问速度，需要在映射表中保存尽量多的映射关系。  
而在Linux中，内存都是以页的形式划分的，默认情况下每页是4K，这就意味着如果物理内存很大，则映射表的条目将会非常多，会影响CPU的检索效率。因为内存大小是固定的，为了减少映射表的条目，可采取的办法只有增加页的尺寸。  
HugePages是在Linux2.6内核被引入的，主要提供4k的page和比较大的page的选择。  

| 概念 | 概念说明 |
| :-- | :-- |
| page table | page table是操作系统上的虚拟内存系统的数据结构模型，用于存储虚拟地址与物理地址的对应关系。<br/>当我们访问内存时，首先访问page table，然后Linux在通过page table的mapping来访问真实物理内存（ram+swap） |
| TLB | A Translation Lookaside Buffer (TLB)<br/>TLB是在cpu中分配的一个固定大小的buffer(or cache)，用于保存page table的部分内容，使CPU更快的访问并进行地址转换。 |
| hugetlb | hugetlb 是记录在TLB 中的条目并指向Hugepages。 |
| hugetlbfs | 这是一个新的基于2.6 kernel之上的内存文件系统，如同tmpfs。<br/>在TLB中通过hugetlb来指向hugepage。这些被分配的hugepage作为内存文件系统hugetlbfs(类似tmpfs)提供给进程使用。 |

HugePages是linux内核的一个特性，使用hugepage可以用更大的内存页来取代传统的4K页面。使用HugePage主要带来以下好处：  
1. HugePages 会在系统启动时，直接分配并保留对应大小的内存区域。  
2. HugePages 在开机之后，如果没有管理员的介入，是不会释放和改变的。  
3. 没有swap。  
4. 大大提高了CPU cache中存放的page table所覆盖的内存大小，从而提高了TLB命中率。  
5. 减轻page table的负载。  
6. 提高内存的性能，降低CPU负载，原理同上。

**为 Java 应用设置大页面**  
在你的启动参数中增加-XX:+UseLargePages即可启用大页面，如果你还想为元空间启用大页面，则需要增加XX:+UseLargePagesInMetaspace。

对于内存占用较大的应用，启用大页面后你会观察到它的内存占用量大幅下降，这是正常的，因为通过大页面占用的内存是不统计在进程的res中的。

如果你看到类似下面这种警告：
```
Java HotSpot(TM) 64-Bit Server VM warning: Failed to reserve large pages memory req_addr: 0x0000000000000000 bytes: 8388608 (errno = 12).
```
这说明你的大页面数量不足，你可以尝试降低Java堆大小或增加大页面数来解决

>[使用HugePages优化内存性能](https://blog.csdn.net/wenshuangzhu/article/details/44095565)
>[为 Java 启用大页面支持](https://blog.cat73.org/20190808/2019080801.jvm-hugepages/)



